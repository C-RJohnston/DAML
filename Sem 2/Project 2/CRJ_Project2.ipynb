{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDAML Report: Parameter Estimation\n",
    "This checkpoint is based around a particle decay from which you will measure a parameter related\n",
    "to the matter/anti-matter asymmetry of the Universe.\n",
    "This relevant decay X → D has the following PDF:\n",
    "P(t; V, τ, ∆ms) ∝ (1 + V sin(∆mt)) × exp(−t/τ )\n",
    "where\n",
    "- t is the observable quantity - the decay time of each decay;\n",
    "- τ is a lifetime parameter;\n",
    "- V is a parameter which measures matter/anti-matter asymmetry and has the value zero if\n",
    "the universe is symmetric (which we know it isnt !);\n",
    "- ∆m is a mass difference parameter which leads to sinusoidal oscillations superimposed on\n",
    "the exponential decay.\n",
    "The nominal values of the parameters are\n",
    "- τ = 1.5\n",
    "- V = 0.1\n",
    "- ∆m = 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "from iminuit import Minuit\n",
    "from iminuit.cost import UnbinnedNLL\n",
    "from iminuit.cost import BinnedNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf as a function\n",
    "def pdf(t, v, tau, m, t_min, t_max):\n",
    "    # get the scaling using scipy integrate\n",
    "    scaling = integrate.quad(lambda t: (1 + v * np.sin(m * t)) * np.exp(- t / tau), t_min, t_max)[0]\n",
    "    return (1 + v * np.sin(m * t)) * np.exp(- t / tau) / scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integral of the pdf over the whole range: 1.0\n"
     ]
    }
   ],
   "source": [
    "# check if the pdf is normalised, should be very close to 1\n",
    "print(\"integral of the pdf over the whole range:\",integrate.quad(lambda x: pdf(x, 0.1, 1.5, 20, 0, 10), 0, 10)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf of the pdf\n",
    "def cdf(ts, v, tau, m, t_min, t_max):\n",
    "    return np.cumsum(pdf(ts, v, tau, m, t_min, t_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Estimating statistical precision [4 marks]\n",
    "Use the method of pseudo-experiments (toy Monte Carlo) to determine the expected statistical\n",
    "precision with which one could measure each of the parameters with \n",
    "(i) 10,000 events and \n",
    "(ii) 100,000 events.\n",
    "Assume perfect detector-resolution/perfect time measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define monte carlo method\n",
    "def box_method(tau, v, m, runs, t_min, t_max):\n",
    "    f_max = 5\n",
    "    # using the box method to generate values from the pdf\n",
    "    ## generate random numbers uniformly [t_min, t_max]\n",
    "    ts = []\n",
    "    while len(ts) < runs:\n",
    "        x = np.random.uniform(t_min, t_max, runs)\n",
    "        ## compute the pdf for these\n",
    "        y1_s = pdf(x, v, tau, m, t_min, t_max)\n",
    "        ## generate a second set of random numbers uniformly [0, f_max]\n",
    "        y2_s = np.random.uniform(0, f_max, runs)\n",
    "        x = x[y2_s < y1_s]\n",
    "        ts.extend(x)\n",
    "    if len(ts) > runs:\n",
    "        return ts[:runs]\n",
    "    return ts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a fit for 10_000 runs of the monte carlo method\n",
    "## create data\n",
    "tau = 1.5\n",
    "v = 0.1\n",
    "m = 20\n",
    "t_min = 0\n",
    "t_max = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_fitting(runs):\n",
    "    data = box_method(tau, v, m, runs, t_min, t_max)\n",
    "    binned_data, bins = np.histogram(data, bins=100, density=True)\n",
    "    bin_width = np.diff(bins)[0]\n",
    "    # cost = UnbinnedNLL(data, pdf*bin_width)\n",
    "    cost = BinnedNLL(binned_data, bins, cdf)\n",
    "    minuit = Minuit(cost, tau=1.5, v=0.1, m=20, t_min=t_min, t_max=t_max)\n",
    "    minuit.errordef=0.5\n",
    "    minuit.fixed['t_min'] = True\n",
    "    minuit.fixed['t_max'] = True\n",
    "    minuit.migrad()\n",
    "    minuit.hesse()\n",
    "    # display the estimations and the errors\n",
    "    for p, val, e in zip(minuit.parameters, minuit.values, minuit.errors):\n",
    "        # round to 2 sig fig\n",
    "        val = np.format_float_positional(val, precision=2, unique=False, fractional=False, trim='k')\n",
    "        e = np.format_float_positional(e, precision=2, unique=False, fractional=False, trim='k')\n",
    "        print(f\"{p} = {val} +/- {e}\")\n",
    "    # store tau and error for later\n",
    "    tau_idx = minuit.parameters.index('tau')\n",
    "    return minuit.values[tau_idx], minuit.errors[tau_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters and errors for 10,000 events:\n",
      "v = 0.0048 +/- 0.39\n",
      "tau = 1.6 +/- 0.38\n",
      "m = 19. +/- 9.7\n",
      "t_min = 0.0 +/- 0.10\n",
      "t_max = 20. +/- 0.20\n",
      "Parameter fitting and eror for 100,000 events:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gamer2021\\anaconda3\\envs\\daml\\lib\\site-packages\\ipykernel_launcher.py:4: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Gamer2021\\anaconda3\\envs\\daml\\lib\\site-packages\\iminuit\\cost.py:30: RuntimeWarning: invalid value encountered in log\n",
      "  return n * (np.log(n + 1e-323) - np.log(mu + 1e-323))\n",
      "C:\\Users\\Gamer2021\\anaconda3\\envs\\daml\\lib\\site-packages\\ipykernel_launcher.py:4: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Gamer2021\\anaconda3\\envs\\daml\\lib\\site-packages\\ipykernel_launcher.py:4: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = -0.31 +/- 0.59\n",
      "tau = 1.7 +/- 0.54\n",
      "m = 36. +/- 0.63\n",
      "t_min = 0.0 +/- 0.10\n",
      "t_max = 20. +/- 0.20\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameters and errors for 10,000 events:\")\n",
    "unbiased_tau, tau_error = parameter_fitting(10_000)\n",
    "print(\"Parameter fitting and eror for 100,000 events:\")\n",
    "parameter_fitting(100_000)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Estimating possible bias due to time resolution [3 marks]\n",
    "In reality the decay time is measured with a resolution (random error) with a standard deviation\n",
    "of σ = fτ where f is some fraction. What this means is that if the true decay time is t_true,\n",
    "then it is actually measured as t where t is distributed around t_true with a Gaussian probability\n",
    "distribution with standard deviation σ.\n",
    "\n",
    "Determine the bias which will be introduced to the measurement of each of the parameters, when\n",
    "the data is subject to the resolution effect, but this is not included into the PDF used for fitting\n",
    "(measuring) the parameters.\n",
    "\n",
    "You should do this for the case of a 10,000 event data sample. Use both f = 0.01 and f = 0.03 and\n",
    "in each case compare the bias (if any) to the expected statistical precision.\n",
    "\n",
    "*Hint: this means producing monte-carlo data where you include the effect of resolution on the decay times\n",
    "you create randomly, but then fitting to it without allowing for this in the PDF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the monte carlo method for tau with random error\n",
    "def biased_box_method(tau, v, m, runs, t_min, t_max, f):\n",
    "    f_max = 5\n",
    "    tau = norm.pdf(f * tau)\n",
    "    # using the box method to generate values from the pdf\n",
    "    ## generate random numbers uniformly [t_min, t_max]\n",
    "    ts = []\n",
    "    while len(ts) < runs:\n",
    "        x = np.random.uniform(t_min, t_max, runs)\n",
    "        ## compute the pdf for these\n",
    "        y1_s = pdf(x, v, tau, m, t_min, t_max)\n",
    "        ## generate a second set of random numbers uniformly [0, f_max]\n",
    "        y2_s = np.random.uniform(0, f_max, runs)\n",
    "        x = x[y2_s < y1_s]\n",
    "        ts.extend(x)\n",
    "    if len(ts) > runs:\n",
    "        return ts[:runs]\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_parameter_fitting(runs, f):\n",
    "    data = biased_box_method(tau, v, m, 10_000, t_min, t_max, f)\n",
    "    binned_data, bins = np.histogram(data, bins=100, density=True)\n",
    "    bin_width = np.diff(bins)[0]\n",
    "    # cost = UnbinnedNLL(data, pdf*bin_width)\n",
    "    cost = BinnedNLL(binned_data, bins, cdf)\n",
    "    minuit = Minuit(cost, tau=1.5, v=0.1, m=20, t_min=t_min, t_max=t_max)\n",
    "    minuit.errordef=0.5\n",
    "    minuit.fixed['t_min'] = True\n",
    "    minuit.fixed['t_max'] = True\n",
    "    minuit.migrad()\n",
    "    minuit.hesse()\n",
    "    # return tau\n",
    "    tau_idx = minuit.parameters.index('tau')\n",
    "    return minuit.values[tau_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias for f=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gamer2021\\anaconda3\\envs\\daml\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n",
      "C:\\Users\\Gamer2021\\anaconda3\\envs\\daml\\lib\\site-packages\\ipykernel_launcher.py:4: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12822005086006\n",
      "Bias for f=0.03\n",
      "1.132088122893335\n",
      "tau error: 0.37616042978925407\n"
     ]
    }
   ],
   "source": [
    "# perform a fit for 10_000 runs of the monte carlo method\n",
    "## create data\n",
    "tau = 1.5\n",
    "v = 0.1\n",
    "m = 20\n",
    "t_min = 0\n",
    "t_max = 20\n",
    "\n",
    "print(\"Bias for f=0.01\")\n",
    "print(np.abs(unbiased_tau - biased_parameter_fitting(10_000, 0.01)))\n",
    "print(\"Bias for f=0.03\")\n",
    "print(np.abs(unbiased_tau - biased_parameter_fitting(10_000, 0.03)))\n",
    "print(\"tau error:\", tau_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Estimating a systematic error due to time acceptance [3 marks]\n",
    "The method of measuring decay-time (i.e. in some detector) is thought to exhibit a decay-time\n",
    "acceptance given by\n",
    "\n",
    "a(t) = (1 + st)\n",
    "\n",
    "where s is only known with a precision of s = 0 ± 0.03\n",
    "\n",
    "Determine a suitable systematic error to assign to the measurement of each of the parameters\n",
    "due to this this limited knowledge of a(t) and in each case compare the this systematic error (if\n",
    "any) to the expected statistical precision.\n",
    "\n",
    "*Hint: This is a stretch part of the project. You will need to understand all of the slides on acceptance in\n",
    "the notes to do this. For this part you will have to generate data and fit it twice. This means you will have\n",
    "to include the acceptance function in the PDF and in its normalisation.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the pdf to include the acceptance\n",
    "def acceptance_pdf(t, v, tau, m, s, t_min, t_max):\n",
    "    # get the scaling using scipy integrate\n",
    "    scaling = integrate.quad(lambda t: (1 + v * np.sin(m * t)) * np.exp(- t / tau) * (1 + s*t), t_min, t_max)[0]\n",
    "    return (1 + v * np.sin(m * t)) * np.exp(- t / tau) * (1 + s * t) / scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptance_cdf(t, v, tau, m, s, t_min, t_max):\n",
    "    return np.cumsum(acceptance_pdf(t, v, tau, m, s, t_min, t_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral of the acceptance pdf: 0.9999999999999997\n"
     ]
    }
   ],
   "source": [
    "# test for unity\n",
    "print(\"Integral of the acceptance pdf:\", integrate.quad(lambda t: acceptance_pdf(t, v, tau, m, 0.03, 0, 20), 0, 20)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = -0.16 +/- 0.41 +/- 0.0055\n",
      "tau = 1.7 +/- 0.5 +/- 0.38\n",
      "m = 20. +/- 1.7 +/- 0.00064\n"
     ]
    }
   ],
   "source": [
    "# generate data with time acceptance\n",
    "## modify box method the use acceptance pdf\n",
    "def acceptance_box_method(tau, v, m, s, runs, t_min, t_max):\n",
    "    f_max = 5\n",
    "    # using the box method to generate values from the pdf\n",
    "    ## generate random numbers uniformly [t_min, t_max]\n",
    "    ts = []\n",
    "    while len(ts) < runs:\n",
    "        x = np.random.uniform(t_min, t_max, runs)\n",
    "        ## compute the pdf for these\n",
    "        y1_s = acceptance_pdf(x, v, tau, m, s, t_min, t_max)\n",
    "        ## generate a second set of random numbers uniformly [0, f_max]\n",
    "        y2_s = np.random.uniform(0, f_max, runs)\n",
    "        x = x[y2_s < y1_s]\n",
    "        ts.extend(x)\n",
    "    if len(ts) > runs:\n",
    "        return ts[:runs]\n",
    "    return ts\n",
    "\n",
    "data = acceptance_box_method(tau, v, m, 0.03, 10_000, 0, 20)\n",
    "# modify parameter fitting to include acceptance\n",
    "def acceptance_parameter_fitting(data):\n",
    "    binned_data, bins = np.histogram(data, bins=100, density=True)\n",
    "    cost = BinnedNLL(binned_data, bins, acceptance_cdf)\n",
    "    minuit = Minuit(cost, tau=1.5, v=0.1, m=20, s=0.03, t_min=t_min, t_max=t_max)\n",
    "    minuit.errordef=0.5\n",
    "    minuit.fixed['t_min'] = True\n",
    "    minuit.fixed['t_max'] = True\n",
    "    minuit.migrad()\n",
    "    minuit.hesse()\n",
    "    return np.array(minuit.values)[:3], np.array(minuit.errors)[:3]\n",
    "\n",
    "def parameter_fitting(data):\n",
    "    binned_data, bins = np.histogram(data, bins=100, density=True)\n",
    "    cost = BinnedNLL(binned_data, bins, cdf)\n",
    "    minuit = Minuit(cost, tau=1.5, v=0.1, m=20, t_min=t_min, t_max=t_max)\n",
    "    minuit.errordef=0.5\n",
    "    minuit.fixed['t_min'] = True\n",
    "    minuit.fixed['t_max'] = True\n",
    "    minuit.migrad()\n",
    "    minuit.hesse()\n",
    "    return minuit.parameters[:3], np.array(minuit.values)[:3], np.array(minuit.errors)[:3]\n",
    "\n",
    "acceptance_vals, _ = acceptance_parameter_fitting(data)\n",
    "params, vals, error = parameter_fitting(data)\n",
    "# calculate the shift as systematic error\n",
    "shift = np.abs(acceptance_vals - vals)\n",
    "for p, val, e, e_s in zip(params, vals, error, shift):\n",
    "    val = np.format_float_positional(val, precision=2, unique=False, fractional=False, trim='k')\n",
    "    e = np.format_float_positional(e, precision=2, unique=False, fractional=False, trim='k')\n",
    "    es = np.format_float_positional(e_s, precision=2, unique=False, fractional=False, trim='k')\n",
    "    print(f\"{p} = {val} +/- {e} +/- {es}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdcfdf091118f8fb4a045e4114f636919db3f7afe5ce983920d73b8d7a8d9376"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('daml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
